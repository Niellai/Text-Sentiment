{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/acer/Documents/PythonProjects/SentimentMining/Corpus'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from os import getcwd\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline    \n",
    "\n",
    "data_dir = os.path.join(getcwd(), 'Corpus')\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded:\n",
      "/home/acer/Documents/PythonProjects/SentimentMining/Corpus/YelpReview_SG_new.json\n"
     ]
    }
   ],
   "source": [
    "# saving current dataframe into csv\n",
    "file_name = os.path.join(data_dir, \"YelpReview_SG_new.json\")\n",
    "if not os.path.exists(file_name):\n",
    "    SG_DF.to_json(file_name)\n",
    "    print(\"File saved:\\n{}\".format(file_name))\n",
    "else:\n",
    "    SG_DF = pd.read_json(file_name, encoding='iso-8859-1') \n",
    "    print(\"File loaded:\\n{}\".format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>biz_id</th>\n",
       "      <th>country</th>\n",
       "      <th>friend_count</th>\n",
       "      <th>latitude</th>\n",
       "      <th>listed_category</th>\n",
       "      <th>longitude</th>\n",
       "      <th>overall_review</th>\n",
       "      <th>pred_sentiment</th>\n",
       "      <th>restaurant_address</th>\n",
       "      <th>restaurant_area</th>\n",
       "      <th>restaurant_name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>review_date</th>\n",
       "      <th>sentitment</th>\n",
       "      <th>url</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_overall_rating</th>\n",
       "      <th>user_review</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>1088</td>\n",
       "      <td>n-Pgwot9rVt7o5Nz3ChpuQ</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>2983</td>\n",
       "      <td>1.315372</td>\n",
       "      <td>Cafes, Pakistani, Indian</td>\n",
       "      <td>103.895449</td>\n",
       "      <td>7 reviews</td>\n",
       "      <td>1</td>\n",
       "      <td>970 Geylang Rd, Singapore 423492</td>\n",
       "      <td>Joo Chiat</td>\n",
       "      <td>Traditional Haig Road Putu Piring</td>\n",
       "      <td>423</td>\n",
       "      <td>21/2/2017</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.yelp.com/biz/Traditional-Haig-Road...</td>\n",
       "      <td>Edinburgh, United Kingdom</td>\n",
       "      <td>5</td>\n",
       "      <td>So I bookmarked this place when I was last in ...</td>\n",
       "      <td>Chantelle T.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>640</td>\n",
       "      <td>t88UJ1i5c2HxbWQTV440mg</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>0</td>\n",
       "      <td>1.296448</td>\n",
       "      <td>French</td>\n",
       "      <td>103.853731</td>\n",
       "      <td>28 reviews</td>\n",
       "      <td>1</td>\n",
       "      <td>2 Stamford Road, Singapore 178882</td>\n",
       "      <td>Bras Brasah</td>\n",
       "      <td>Jaan</td>\n",
       "      <td>11</td>\n",
       "      <td>17/4/2013</td>\n",
       "      <td>-1</td>\n",
       "      <td>https://www.yelp.com/biz/Jaan-Singapore</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>3</td>\n",
       "      <td>Interesting but felt chef was trying to hard. ...</td>\n",
       "      <td>Brian H.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>558</td>\n",
       "      <td>j_aIYB4uGOR45jAXivFNZQ</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>145</td>\n",
       "      <td>1.305307</td>\n",
       "      <td>Chinese, Noodles</td>\n",
       "      <td>103.862343</td>\n",
       "      <td>72 reviews</td>\n",
       "      <td>1</td>\n",
       "      <td>466 Crawford Lane, Singapore 190465</td>\n",
       "      <td>Lavender</td>\n",
       "      <td>Tai Wah Pork Noodle</td>\n",
       "      <td>88</td>\n",
       "      <td>28/8/2014</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.yelp.com/biz/Hill-Street-Tai-Hwa-P...</td>\n",
       "      <td>Bukit Timah, Singapore, Singapore</td>\n",
       "      <td>5</td>\n",
       "      <td>Definitely one of the best Mee Pok I have tast...</td>\n",
       "      <td>Jahan L.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>282</td>\n",
       "      <td>b1ytpJcakiLtlpGkvxal-Q</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>276</td>\n",
       "      <td>1.304335</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>103.836174</td>\n",
       "      <td>17 reviews</td>\n",
       "      <td>1</td>\n",
       "      <td>290 Orchard Road, Singapore 238859</td>\n",
       "      <td>Orchard</td>\n",
       "      <td>Crystal Jade Golden Palace</td>\n",
       "      <td>178</td>\n",
       "      <td>29/1/2015</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.yelp.com/biz/Crystal-Jade-Golden-P...</td>\n",
       "      <td>Singapore, Singapore</td>\n",
       "      <td>4</td>\n",
       "      <td>It has been a few years since I revisited this...</td>\n",
       "      <td>Keira H.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>456</td>\n",
       "      <td>cMZ7ZjrrnRxt2InvCxmNrA</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>35</td>\n",
       "      <td>1.296551</td>\n",
       "      <td>Bars, Italian</td>\n",
       "      <td>103.855047</td>\n",
       "      <td>14 reviews</td>\n",
       "      <td>1</td>\n",
       "      <td>36 Purvis St, Singapore 188613</td>\n",
       "      <td>Bugis, Bras Brasah, City Hall</td>\n",
       "      <td>Garibaldi</td>\n",
       "      <td>50</td>\n",
       "      <td>4/11/2015</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.yelp.com/biz/Garibaldi-Singapore</td>\n",
       "      <td>Singapore, Singapore</td>\n",
       "      <td>4</td>\n",
       "      <td>I went there during the Restaurant Week on 1 N...</td>\n",
       "      <td>Ivy C.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                  biz_id     country  friend_count  latitude  \\\n",
       "1088        1088  n-Pgwot9rVt7o5Nz3ChpuQ   Singapore          2983  1.315372   \n",
       "640          640  t88UJ1i5c2HxbWQTV440mg   Singapore             0  1.296448   \n",
       "558          558  j_aIYB4uGOR45jAXivFNZQ   Singapore           145  1.305307   \n",
       "282          282  b1ytpJcakiLtlpGkvxal-Q   Singapore           276  1.304335   \n",
       "456          456  cMZ7ZjrrnRxt2InvCxmNrA   Singapore            35  1.296551   \n",
       "\n",
       "               listed_category   longitude overall_review  pred_sentiment  \\\n",
       "1088  Cafes, Pakistani, Indian  103.895449      7 reviews               1   \n",
       "640                     French  103.853731     28 reviews               1   \n",
       "558           Chinese, Noodles  103.862343     72 reviews               1   \n",
       "282                    Chinese  103.836174     17 reviews               1   \n",
       "456              Bars, Italian  103.855047     14 reviews               1   \n",
       "\n",
       "                       restaurant_address                restaurant_area  \\\n",
       "1088     970 Geylang Rd, Singapore 423492                      Joo Chiat   \n",
       "640     2 Stamford Road, Singapore 178882                    Bras Brasah   \n",
       "558   466 Crawford Lane, Singapore 190465                       Lavender   \n",
       "282    290 Orchard Road, Singapore 238859                        Orchard   \n",
       "456        36 Purvis St, Singapore 188613  Bugis, Bras Brasah, City Hall   \n",
       "\n",
       "                        restaurant_name  review_count review_date  sentitment  \\\n",
       "1088  Traditional Haig Road Putu Piring           423   21/2/2017           1   \n",
       "640                                Jaan            11   17/4/2013          -1   \n",
       "558                 Tai Wah Pork Noodle            88   28/8/2014           1   \n",
       "282          Crystal Jade Golden Palace           178   29/1/2015           1   \n",
       "456                           Garibaldi            50   4/11/2015           1   \n",
       "\n",
       "                                                    url  \\\n",
       "1088  https://www.yelp.com/biz/Traditional-Haig-Road...   \n",
       "640             https://www.yelp.com/biz/Jaan-Singapore   \n",
       "558   https://www.yelp.com/biz/Hill-Street-Tai-Hwa-P...   \n",
       "282   https://www.yelp.com/biz/Crystal-Jade-Golden-P...   \n",
       "456        https://www.yelp.com/biz/Garibaldi-Singapore   \n",
       "\n",
       "                          user_location  user_overall_rating  \\\n",
       "1088          Edinburgh, United Kingdom                    5   \n",
       "640                         Houston, TX                    3   \n",
       "558   Bukit Timah, Singapore, Singapore                    5   \n",
       "282                Singapore, Singapore                    4   \n",
       "456                Singapore, Singapore                    4   \n",
       "\n",
       "                                            user_review      username  \n",
       "1088  So I bookmarked this place when I was last in ...  Chantelle T.  \n",
       "640   Interesting but felt chef was trying to hard. ...      Brian H.  \n",
       "558   Definitely one of the best Mee Pok I have tast...      Jahan L.  \n",
       "282   It has been a few years since I revisited this...      Keira H.  \n",
       "456   I went there during the Restaurant Week on 1 N...        Ivy C.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SG_DF.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process: 1243/1243\n"
     ]
    }
   ],
   "source": [
    "# convert all documents into tokens using Spacy\n",
    "nlp = spacy.load('en')\n",
    "stopWords = [\"place\", \"just\", \"add\", \"big\", \"want\", \"pax\", \"quit\", \"everyone\", \"aaa\", \"aaaaammaz\", \"come\", \"ask\"]\n",
    "\n",
    "def process_text(doc1):\n",
    "    doc1 = doc1.lower()\n",
    "    doc1 = nlp(doc1)    \n",
    "    doc1 = [token for token in doc1 if token.is_alpha]\n",
    "    doc1 = [token for token in doc1 if not token.is_stop]\n",
    "    doc1 = [token for token in doc1 if not token.text in stopWords]\n",
    "    doc1 = [token for token in doc1 if not token.is_punct]\n",
    "    doc1 = [token for token in doc1 if not token.is_space]\n",
    "    doc1 = [token.lemma_ for token in doc1]\n",
    "    doc1 = [token for token in doc1 if len(token) >= 3]\n",
    "    doc1 = \" \".join(doc1)\n",
    "    return doc1.replace(\".\", \" \")\n",
    "   \n",
    "clean_docs = []\n",
    "for idx, row in SG_DF.iterrows():\n",
    "    doc = process_text(row['user_review'])\n",
    "    clean_docs.append(doc)\n",
    "    clear_output(wait=True)\n",
    "    print(\"Process: {}/{}\".format(len(clean_docs), len(SG_DF)))\n",
    "\n",
    "# # Test use\n",
    "# doc = \"This is a test text, good place, just eat.\"\n",
    "# process_text(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert document to vector, either as a unigram or ngram\n",
    "vectorizer_unigram = CountVectorizer(analyzer='word', stop_words='english').fit(clean_docs)\n",
    "vectorizer_ngram = CountVectorizer(analyzer='word', ngram_range=(1,3), stop_words='english').fit(clean_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram vocab size: 8449\n",
      "N-gram vocab size: 152171\n"
     ]
    }
   ],
   "source": [
    "# Number of vocabulary in CountVectorizer\n",
    "print(\"Unigram vocab size: {}\".format(len(vectorizer_unigram.vocabulary_)))\n",
    "print(\"N-gram vocab size: {}\".format(len(vectorizer_ngram.vocabulary_)))\n",
    "# vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1243, 152171)\n",
      "Y: (1243,)\n"
     ]
    }
   ],
   "source": [
    "# Transform all documents into vectors\n",
    "X = vectorizer_ngram.transform(SG_DF['user_review'])\n",
    "Y = SG_DF['sentitment']\n",
    "\n",
    "print(\"X: {}\".format(X.shape))\n",
    "print(\"Y: {}\".format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Multinomial Naive Bayes</h3>\n",
    "Training a Multinomial Naive Bayes model to classify user sentiment value. \n",
    "\n",
    "Naive bayes is a classifier which is based on Bayes theorem. It uses words proabilities occuring on one after another and assume conditional independence.\n",
    "\n",
    "**Multinomial NB**, is a variation takes into account the number of occurrences of term t in training document, including multiple occurrences.\n",
    "\n",
    "Reference:  \n",
    "[http://blog.datumbox.com/machine-learning-tutorial-the-naive-bayes-text-classifier/](http://blog.datumbox.com/machine-learning-tutorial-the-naive-bayes-text-classifier/)  \n",
    "[https://medium.com/@theflyingmantis/text-classification-in-nlp-naive-bayes-a606bf419f8c](https://medium.com/@theflyingmantis/text-classification-in-nlp-naive-bayes-a606bf419f8c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (870, 152171)\n",
      "Test size: (373, 152171)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Train size: {}\".format(X_train.shape))\n",
    "print(\"Test size: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'alpha': [0.5, 0.8, 1]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nb_model = MultinomialNB()\n",
    "# nb_model.fit(X_train, Y_train)\n",
    "\n",
    "# searching for best parameter using GridSearch\n",
    "param_grid = {\"alpha\": [0.5, 0.8, 1]}\n",
    "nb_model = MultinomialNB()\n",
    "grid = GridSearchCV(estimator=nb_model, param_grid=param_grid, n_jobs=-1)\n",
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7862068965517242\n",
      "{'alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#ALthough grid search find the best value for alpha=0.5, when alpha=1 have higher F1 score\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When alpha = 0.5 <br>\n",
    "TP: 80 <br>\n",
    "FP: 22 <br>\n",
    "FN: 8 <br>\n",
    "Precision: 0.784 <br>\n",
    "Recall: 0.909 <br>\n",
    "F1: 0.842 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>-1</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred     -1    1  All\n",
       "actuals              \n",
       "-1        1   24   25\n",
       "1         0   88   88\n",
       "All       1  112  113"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model = MultinomialNB(alpha=1)\n",
    "nb_model.fit(X_train, Y_train)\n",
    "\n",
    "nb_train_preds = nb_model.predict(X_train)\n",
    "preds = nb_model.predict(X_test)\n",
    "# print(preds)\n",
    "\n",
    "confm = pd.crosstab( pd.Series(Y_test), pd.Series(preds), rownames= ['actuals'], colnames=['pred'], margins=True)\n",
    "confm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 88\n",
      "FP: 24\n",
      "FN: 0\n",
      "TN: 1\n",
      "\n",
      "Precision: 0.786\n",
      "Recall: 1.0\n",
      "Specificity: 0.04\n",
      "F1: 0.88\n"
     ]
    }
   ],
   "source": [
    "print(\"TP: {}\".format(confm[1][1]))\n",
    "print(\"FP: {}\".format(confm[1][-1]))\n",
    "print(\"FN: {}\".format(confm[-1][1]))\n",
    "print(\"TN: {}\".format(confm[-1][-1]))\n",
    "print()\n",
    "\n",
    "precision = (confm[1][1]) / (confm[1][1] + (confm[1][-1]))\n",
    "print (\"Precision: {}\".format(str(round(precision, 3))))\n",
    "\n",
    "recall = (confm[1][1]) / (confm[1][1] + (confm[-1][1]))\n",
    "print (\"Recall: {}\".format(str(round(recall, 3))))\n",
    "\n",
    "specificity = (confm[-1][-1] / (confm[-1][-1] + confm[1][-1]))\n",
    "print (\"Specificity: {}\".format(str(round(specificity, 3))))\n",
    "\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "print(\"F1: {}\".format(str(round(f1, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encounter a very small value in specificity, this might be due to inbalance dataset. We will verify this later in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>NB - Comparing unigram and n-gram results</h2>\n",
    "<h3>Unigram results</h3>  \n",
    "TP: 80  \n",
    "FP: 23  \n",
    "FN: 8  \n",
    "TN: 2  \n",
    "\n",
    "Precision: 0.777  \n",
    "Recall: 0.909  \n",
    "Specificity: 0.08  \n",
    "F1: 0.838  \n",
    "\n",
    "<h3>N-gram results</h3>  \n",
    "TP: 88  \n",
    "FP: 24  \n",
    "FN: 0  \n",
    "TN: 1  \n",
    "\n",
    "Precision: 0.786  \n",
    "Recall: 1.0  \n",
    "Specificity: 0.04  \n",
    "F1: 0.88 \n",
    "\n",
    "Because of NB uses conditional independent, ngrams tokens captures more useful sequences of words that helps in estimating sentiment. Example negation of sentiment cannot be capture by unigram token.  \n",
    "(\"taste\", \"not\", \"good\")  \n",
    "(\"taste not\", \"not good\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['food',\n",
       " 'good',\n",
       " 'like',\n",
       " 'restaurant',\n",
       " 'singapore',\n",
       " 'dish',\n",
       " 'service',\n",
       " 'great',\n",
       " 'place',\n",
       " 'best']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_words(vectorizer, model, n=10):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    sorted_coef = sorted(zip(model.coef_[0], feature_names), key=lambda k:k[0], reverse=True)\n",
    "    return [w[1] for w in sorted_coef[:n]]\n",
    "\n",
    "# Top 10 feature words using Naive Bayes Model\n",
    "feature_words(vectorizer, nb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaaaammaz',\n",
       " 'abalone',\n",
       " 'abbey',\n",
       " 'abd',\n",
       " 'abierta',\n",
       " 'abroad',\n",
       " 'abs',\n",
       " 'absolutely',\n",
       " 'abstract',\n",
       " 'absurdly']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def least_feature_words(vectorizer, model, n=10):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    sorted_coef = sorted(zip(model.coef_[0], feature_names), key=lambda k:k[0], reverse=False)\n",
    "    return [w[1] for w in sorted_coef[:n]]\n",
    "\n",
    "least_feature_words(vectorizer, nb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classification using SVM</h3>\n",
    "Training a SVM model to classify user sentiment value.  \n",
    "\n",
    "![image1.png](https://66.media.tumblr.com/ff709fe1c77091952fb3e3e6af91e302/tumblr_inline_o9aa8dYRkB1u37g00_540.png)\n",
    "\n",
    "Unlike to NB which uses words probablitlies, SVM uses vector space for classification. SVM uses support vectors which are data points closest to the hyperplane to identify the best separation on different classes. When there is no clear hyperplane to separate classes. SVM \"transform\" data point to higher dimensions to find the best separation.  \n",
    "![image2.png](https://66.media.tumblr.com/9bffea56372d28d2a30f80557451e824/tumblr_inline_o9aabehtqP1u37g00_540.png)\n",
    "\n",
    "References:  \n",
    "[https://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.html](https://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'C': [0.002, 0.005, 0.01, 0.05, 0.1, 0.3, 0.6, 0.9, 1], 'loss': ['hinge', 'squared_hinge'], 'multi_class': ['ovr', 'crammer_singer']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search\n",
    "param_grid = {\"C\": [0.002, 0.005, 0.01, 0.05, 0.1, 0.3, 0.6, 0.9, 1],\n",
    "             \"loss\": [\"hinge\", \"squared_hinge\"],\n",
    "             \"multi_class\": ['ovr', 'crammer_singer']}\n",
    "svm_model = svm.LinearSVC(random_state=42)\n",
    "grid = GridSearchCV(estimator=svm_model, param_grid=param_grid, n_jobs=-1)\n",
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7862068965517242\n",
      "{'C': 0.1, 'loss': 'squared_hinge', 'multi_class': 'ovr'}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When C=0.002 it have the best performance although GridSearch reference to C=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>-1</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>86</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4</td>\n",
       "      <td>109</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred     -1    1  All\n",
       "actuals              \n",
       "-1        2   23   25\n",
       "1         2   86   88\n",
       "All       4  109  113"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = svm.LinearSVC(C=0.002,random_state=42)\n",
    "svm_model.fit(X_train, Y_train)\n",
    "\n",
    "svm_train_preds = svm_model.predict(X_train)\n",
    "preds = svm_model.predict(X_test)\n",
    "# print(preds)\n",
    "\n",
    "svm_confm = pd.crosstab( pd.Series(Y_test), pd.Series(preds), rownames= ['actuals'], colnames=['pred'],margins=True)\n",
    "svm_confm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 86\n",
      "FP: 23\n",
      "FN: 2\n",
      "TN: 2\n",
      "\n",
      "Precision: 0.789\n",
      "Recall: 0.977\n",
      "Specificity: 0.08\n",
      "F1: 0.873\n"
     ]
    }
   ],
   "source": [
    "print(\"TP: {}\".format(svm_confm[1][1]))\n",
    "print(\"FP: {}\".format(svm_confm[1][-1]))\n",
    "print(\"FN: {}\".format(svm_confm[-1][1]))\n",
    "print(\"TN: {}\".format(svm_confm[-1][-1]))\n",
    "print()\n",
    "\n",
    "precision = (svm_confm[1][1]) / (svm_confm[1][1] + (svm_confm[1][-1]))\n",
    "print (\"Precision: {}\".format(str(round(precision, 3))))\n",
    "\n",
    "recall = (svm_confm[1][1]) / (svm_confm[1][1] + (svm_confm[-1][1]))\n",
    "print (\"Recall: {}\".format(str(round(recall, 3))))\n",
    "\n",
    "specificity = (svm_confm[-1][-1] / (svm_confm[-1][-1] + svm_confm[1][-1]))\n",
    "print (\"Specificity: {}\".format(str(round(specificity, 3))))\n",
    "\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "print(\"F1: {}\".format(str(round(f1, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to NB we have very bad result on specificity. Will verify this at later part of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SVM - Comparing unigram and n-gram results</h2>\n",
    "<h3>Unigram results</h3>  \n",
    "TP: 85  \n",
    "FP: 23  \n",
    "FN: 3  \n",
    "TN: 2  \n",
    "\n",
    "Precision: 0.787  \n",
    "Recall: 0.966  \n",
    "Specificity: 0.08  \n",
    "F1: 0.867  \n",
    "\n",
    "<h3>N-gram results</h3>  \n",
    "TP: 86  \n",
    "FP: 23  \n",
    "FN: 2  \n",
    "TN: 2  \n",
    "\n",
    "Precision: 0.789  \n",
    "Recall: 0.977  \n",
    "Specificity: 0.08  \n",
    "F1: 0.873 \n",
    "\n",
    "SVM uses vectors space adding more collection of tokens does not improve performance as much as NB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best',\n",
       " 'delicious',\n",
       " 'great',\n",
       " 'singapore',\n",
       " 'amazing',\n",
       " 'excellent',\n",
       " 'line',\n",
       " 'recommend',\n",
       " 'service',\n",
       " 'definitely']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 feature words using SVM model\n",
    "feature_words(vectorizer, svm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Saving result to respective format</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 1243\n",
      "[('Chinese', 267), ('Asian Fusion', 154), ('Noodles', 148), ('Steakhouses', 136), ('Singaporean', 125), ('French', 106), ('Indian', 102), ('Spanish', 79), ('Australian', 67), ('Japanese', 67), ('Food Stands', 56), ('Barbeque', 55), ('Thai', 53), ('Italian', 50), ('American (Traditional)', 48), ('Modern European', 47), ('Cocktail Bars', 46), ('Cantonese', 45), ('Dim Sum', 43), ('Tapas/Small Plates', 41), ('Street Vendors', 41), ('Bars', 40), ('Shanghainese', 31), ('Lounges', 28), ('Fondue', 28), ('Seafood', 25), ('Vegetarian', 25), ('Food', 21), ('Mediterranean', 20), ('Ramen', 13), ('Wine Bars', 13), ('Brasseries', 13), ('Breakfast & Brunch', 12), ('Szechuan', 12), ('Restaurants', 11), ('Gelato', 10), ('Ice Cream & Frozen Yogurt', 10), ('Pakistani', 7), ('Venues & Event Spaces', 7), ('Local Flavor', 7), ('Malaysian', 7), ('Nasi Lemak', 7), ('Cafes', 7), ('Imported Food', 6), ('Sushi Bars', 6), ('Farmers Market', 4), ('Mexican', 3), ('Soup', 2), ('Bakeries', 1)]\n",
      "\n",
      "['Chinese', 'Asian Fusion', 'Noodles', 'Steakhouses']\n",
      "\n",
      "Train Records: 26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Noodles</th>\n",
       "      <th>Steakhouses</th>\n",
       "      <th>Chinese</th>\n",
       "      <th>Asian Fusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'name': 'Xing Ji Rou Cuo Mian', 'reviews': [{...</td>\n",
       "      <td>{'name': 'Fat Cow', 'reviews': [{'score': 1, '...</td>\n",
       "      <td>{'name': 'Xing Ji Rou Cuo Mian', 'reviews': [{...</td>\n",
       "      <td>{'name': 'Waku Ghin', 'reviews': [{'score': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'name': 'Depot Road Zhen Shan Mei Claypot Lak...</td>\n",
       "      <td>{'name': 'Burnt Ends', 'reviews': [{'score': 1...</td>\n",
       "      <td>{'name': 'Hup Hup Minced Meat Noodle', 'review...</td>\n",
       "      <td>{'name': 'Wild Rocket', 'reviews': [{'score': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'name': 'Outram Park Fried Kway Teow', 'revie...</td>\n",
       "      <td>{'name': 'Gordon Grill', 'reviews': [{'score':...</td>\n",
       "      <td>{'name': 'Tiong Bahru Hainanese Boneless Chick...</td>\n",
       "      <td>{'name': 'Akira Back', 'reviews': [{'score': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'name': 'Tai Wah Pork Noodle', 'reviews': [{'...</td>\n",
       "      <td>{'name': 'CUT', 'reviews': [{'score': 1, 'text...</td>\n",
       "      <td>{'name': 'Wah Kee Big Prawn Noodles', 'reviews...</td>\n",
       "      <td>{'name': 'Lao Fu Zi Fried Kway Teow', 'reviews...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'name': 'Lor Mee 178', 'reviews': [{'score': ...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Geylang Claypot Rice', 'reviews': [{...</td>\n",
       "      <td>{'name': 'Candlenut', 'reviews': [{'score': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'name': 'Zhong Guo La Mian Xiao Long Bao', 'r...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Jalan Sultan Prawn Mee', 'reviews': ...</td>\n",
       "      <td>{'name': 'True Blue Cuisine', 'reviews': [{'sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Min Jiang At One-North', 'reviews': ...</td>\n",
       "      <td>{'name': 'Empress', 'reviews': [{'score': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Balestier Road Hoover Rojak', 'revie...</td>\n",
       "      <td>{'name': 'Mezza9', 'reviews': [{'score': 1, 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Tian Tian Hainanese Chicken Rice', '...</td>\n",
       "      <td>{'name': 'Labyrinth', 'reviews': [{'score': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Empress', 'reviews': [{'score': 1, '...</td>\n",
       "      <td>{'name': 'Yuan Chun Famous Lor Mee', 'reviews'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Tai Wah Pork Noodle', 'reviews': [{'...</td>\n",
       "      <td>{'name': 'Alma', 'reviews': [{'score': -1, 'te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Hong Heng Fried Sotong Prawn Mee', '...</td>\n",
       "      <td>{'name': 'Majestic', 'reviews': [{'score': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Hill Street Fried Kway Teow', 'revie...</td>\n",
       "      <td>{'name': 'New Lucky Claypot Rice', 'reviews': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'The Blue Ginger', 'reviews': [{'scor...</td>\n",
       "      <td>{'name': '85 Bedok North Fried Oyster', 'revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Whitley Rd Big Prawn Noodle', 'revie...</td>\n",
       "      <td>{'name': 'The Blue Ginger', 'reviews': [{'scor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Lao Fu Zi Fried Kway Teow', 'reviews...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Pin Wei Hong Kong Style Chee Cheong ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Lor Mee 178', 'reviews': [{'score': ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Lei Garden', 'reviews': [{'score': 1...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Candlenut', 'reviews': [{'score': 1,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Crystal Jade Golden Palace', 'review...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Jiang-Nan Chun', 'reviews': [{'score...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Majestic', 'reviews': [{'score': 1, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': '85 Bedok North Fried Oyster', 'revie...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'The Clifford Pier', 'reviews': [{'sc...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'name': 'Roland', 'reviews': [{'score': 1, 't...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Noodles  \\\n",
       "0   {'name': 'Xing Ji Rou Cuo Mian', 'reviews': [{...   \n",
       "1   {'name': 'Depot Road Zhen Shan Mei Claypot Lak...   \n",
       "2   {'name': 'Outram Park Fried Kway Teow', 'revie...   \n",
       "3   {'name': 'Tai Wah Pork Noodle', 'reviews': [{'...   \n",
       "4   {'name': 'Lor Mee 178', 'reviews': [{'score': ...   \n",
       "5   {'name': 'Zhong Guo La Mian Xiao Long Bao', 'r...   \n",
       "6                                                None   \n",
       "7                                                None   \n",
       "8                                                None   \n",
       "9                                                None   \n",
       "10                                               None   \n",
       "11                                               None   \n",
       "12                                               None   \n",
       "13                                               None   \n",
       "14                                               None   \n",
       "15                                               None   \n",
       "16                                               None   \n",
       "17                                               None   \n",
       "18                                               None   \n",
       "19                                               None   \n",
       "20                                               None   \n",
       "21                                               None   \n",
       "22                                               None   \n",
       "23                                               None   \n",
       "24                                               None   \n",
       "25                                               None   \n",
       "\n",
       "                                          Steakhouses  \\\n",
       "0   {'name': 'Fat Cow', 'reviews': [{'score': 1, '...   \n",
       "1   {'name': 'Burnt Ends', 'reviews': [{'score': 1...   \n",
       "2   {'name': 'Gordon Grill', 'reviews': [{'score':...   \n",
       "3   {'name': 'CUT', 'reviews': [{'score': 1, 'text...   \n",
       "4                                                None   \n",
       "5                                                None   \n",
       "6                                                None   \n",
       "7                                                None   \n",
       "8                                                None   \n",
       "9                                                None   \n",
       "10                                               None   \n",
       "11                                               None   \n",
       "12                                               None   \n",
       "13                                               None   \n",
       "14                                               None   \n",
       "15                                               None   \n",
       "16                                               None   \n",
       "17                                               None   \n",
       "18                                               None   \n",
       "19                                               None   \n",
       "20                                               None   \n",
       "21                                               None   \n",
       "22                                               None   \n",
       "23                                               None   \n",
       "24                                               None   \n",
       "25                                               None   \n",
       "\n",
       "                                              Chinese  \\\n",
       "0   {'name': 'Xing Ji Rou Cuo Mian', 'reviews': [{...   \n",
       "1   {'name': 'Hup Hup Minced Meat Noodle', 'review...   \n",
       "2   {'name': 'Tiong Bahru Hainanese Boneless Chick...   \n",
       "3   {'name': 'Wah Kee Big Prawn Noodles', 'reviews...   \n",
       "4   {'name': 'Geylang Claypot Rice', 'reviews': [{...   \n",
       "5   {'name': 'Jalan Sultan Prawn Mee', 'reviews': ...   \n",
       "6   {'name': 'Min Jiang At One-North', 'reviews': ...   \n",
       "7   {'name': 'Balestier Road Hoover Rojak', 'revie...   \n",
       "8   {'name': 'Tian Tian Hainanese Chicken Rice', '...   \n",
       "9   {'name': 'Empress', 'reviews': [{'score': 1, '...   \n",
       "10  {'name': 'Tai Wah Pork Noodle', 'reviews': [{'...   \n",
       "11  {'name': 'Hong Heng Fried Sotong Prawn Mee', '...   \n",
       "12  {'name': 'Hill Street Fried Kway Teow', 'revie...   \n",
       "13  {'name': 'The Blue Ginger', 'reviews': [{'scor...   \n",
       "14  {'name': 'Whitley Rd Big Prawn Noodle', 'revie...   \n",
       "15  {'name': 'Lao Fu Zi Fried Kway Teow', 'reviews...   \n",
       "16  {'name': 'Pin Wei Hong Kong Style Chee Cheong ...   \n",
       "17  {'name': 'Lor Mee 178', 'reviews': [{'score': ...   \n",
       "18  {'name': 'Lei Garden', 'reviews': [{'score': 1...   \n",
       "19  {'name': 'Candlenut', 'reviews': [{'score': 1,...   \n",
       "20  {'name': 'Crystal Jade Golden Palace', 'review...   \n",
       "21  {'name': 'Jiang-Nan Chun', 'reviews': [{'score...   \n",
       "22  {'name': 'Majestic', 'reviews': [{'score': 1, ...   \n",
       "23  {'name': '85 Bedok North Fried Oyster', 'revie...   \n",
       "24  {'name': 'The Clifford Pier', 'reviews': [{'sc...   \n",
       "25  {'name': 'Roland', 'reviews': [{'score': 1, 't...   \n",
       "\n",
       "                                         Asian Fusion  \n",
       "0   {'name': 'Waku Ghin', 'reviews': [{'score': 1,...  \n",
       "1   {'name': 'Wild Rocket', 'reviews': [{'score': ...  \n",
       "2   {'name': 'Akira Back', 'reviews': [{'score': 1...  \n",
       "3   {'name': 'Lao Fu Zi Fried Kway Teow', 'reviews...  \n",
       "4   {'name': 'Candlenut', 'reviews': [{'score': 1,...  \n",
       "5   {'name': 'True Blue Cuisine', 'reviews': [{'sc...  \n",
       "6   {'name': 'Empress', 'reviews': [{'score': 1, '...  \n",
       "7   {'name': 'Mezza9', 'reviews': [{'score': 1, 't...  \n",
       "8   {'name': 'Labyrinth', 'reviews': [{'score': 1,...  \n",
       "9   {'name': 'Yuan Chun Famous Lor Mee', 'reviews'...  \n",
       "10  {'name': 'Alma', 'reviews': [{'score': -1, 'te...  \n",
       "11  {'name': 'Majestic', 'reviews': [{'score': 1, ...  \n",
       "12  {'name': 'New Lucky Claypot Rice', 'reviews': ...  \n",
       "13  {'name': '85 Bedok North Fried Oyster', 'revie...  \n",
       "14  {'name': 'The Blue Ginger', 'reviews': [{'scor...  \n",
       "15                                               None  \n",
       "16                                               None  \n",
       "17                                               None  \n",
       "18                                               None  \n",
       "19                                               None  \n",
       "20                                               None  \n",
       "21                                               None  \n",
       "22                                               None  \n",
       "23                                               None  \n",
       "24                                               None  \n",
       "25                                               None  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train score\n",
    "print(\"Total records: {}\".format(len(SG_DF)))\n",
    "\n",
    "unique_categories = []\n",
    "for idx, doc in SG_DF.iterrows():\n",
    "    for cat in doc['listed_category'].split(','):        \n",
    "        unique_categories.append(cat.strip())\n",
    "\n",
    "unique_categories = set(unique_categories)\n",
    "# print(\"unique_categories: {}\".format(len(unique_categories)))\n",
    "# print(unique_categories)\n",
    "# print()\n",
    "\n",
    "# Count the total number comments on each categories.\n",
    "# Note that categories will overlap.\n",
    "rest_category = dict.fromkeys(unique_categories, 0)\n",
    "for idx, doc in SG_DF.iterrows():\n",
    "    for cat in doc['listed_category'].split(','):\n",
    "        cat = cat.strip()\n",
    "        rest_category[cat] += 1\n",
    "\n",
    "rest_category = sorted(rest_category.items(), key=lambda k: k[1], reverse=True)\n",
    "print(rest_category)\n",
    "print()\n",
    "\n",
    "# Extracting only Chinese, Asian Fusion, Noodles, Steakhouses categories\n",
    "rest_categories = [rest[0] for rest in rest_category[:4]]\n",
    "print(rest_categories)\n",
    "print()\n",
    "\n",
    "extract_SG_DF = {'Chinese': [], 'Asian Fusion':[], 'Noodles':[], 'Steakhouses':[]}\n",
    "\n",
    "# Extracting only Train and Test records\n",
    "Train_DF = [SG_DF.iloc[idx] for idx, label in Y_train.items()]\n",
    "Train_DF = pd.DataFrame(Train_DF)\n",
    "Test_DF = [SG_DF.iloc[idx] for idx, label in Y_test.items()]\n",
    "Test_DF = pd.DataFrame(Test_DF)\n",
    "\n",
    "# placing predicted records back into dataframes\n",
    "Train_DF['svm_train_preds'] = svm_train_preds\n",
    "Train_DF['nb_train_preds'] = nb_train_preds\n",
    "\n",
    "train_scored = {}\n",
    "for cat in rest_categories:        \n",
    "    extract_SG_DF[cat] = [row for idx, row in Train_DF.iterrows() if cat in row['listed_category']]\n",
    "    extract_SG_DF[cat] = pd.DataFrame(extract_SG_DF[cat])\n",
    "    \n",
    "    # retrieve unique restaurant name\n",
    "    rest_list = []\n",
    "    unique_rest_name = set(extract_SG_DF[cat]['restaurant_name'])\n",
    "    for name in unique_rest_name:        \n",
    "        rest = {}\n",
    "        rest['name'] = name        \n",
    "        \n",
    "        # retrieve reviews on each restaurant \n",
    "        review_list = []\n",
    "        single_rest = extract_SG_DF[cat][extract_SG_DF[cat]['restaurant_name'] == name]                \n",
    "        rest['nb_review'] = list(single_rest['overall_review'])[0]\n",
    "        rest['nb_review'] = int(rest['nb_review'].split(' ')[0])\n",
    "        for idx, row in single_rest.iterrows():            \n",
    "            review = {}\n",
    "            review['text'] = row['user_review']\n",
    "            review['score'] = row['svm_train_preds'] #Change to either SVM or NB\n",
    "            review_list.append(review)\n",
    "        rest['reviews'] = review_list\n",
    "        rest_list.append(rest)\n",
    "    \n",
    "    train_scored[cat] = rest_list\n",
    "\n",
    "train_scored = pd.DataFrame.from_dict(train_scored, orient='index')\n",
    "train_scored = train_scored.transpose()\n",
    "\n",
    "print(\"Train Records: {}\".format(len(train_scored)))\n",
    "train_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved. D:\\Google Drive\\NUS Masters\\Sem2\\Sentiment Mining\\CA1\\Corpus\\svm_train_scored.csv\n"
     ]
    }
   ],
   "source": [
    "# saving train_score\n",
    "file_name = os.path.join(data_dir, \"svm_train_scored.csv\")\n",
    "train_scored.to_csv(file_name, sep='\\t', encoding='iso-8859-1')\n",
    "print(\"File saved. {}\".format(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Verifying on poor performance in Specificity</h3>\n",
    "\n",
    "We have found that dataset is skew towards positive sentiment and for 3 stars rating comments, many contain positive sentiment comments. This is further explored in *CA1_LabelData.ipynb*. To verifying if labeling 3 stars rating as negative are affecting Specificity score. 3 stars rating comments are filter and rebalance our positive and negative sentiment comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\himur\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total neg: 113\n",
      "Total pos: 923\n",
      "Train size: (158, 159745)\n",
      "Test size: (68, 159745)\n"
     ]
    }
   ],
   "source": [
    "FILTERED_SG_DF = SG_DF[SG_DF['user_overall_rating'] != 3]\n",
    "pred_rate3 = []\n",
    "\n",
    "# filter alway rating 3\n",
    "for idx, row in FILTERED_SG_DF.iterrows():\n",
    "    score = row['user_overall_rating']\n",
    "    if score <3:\n",
    "        pred_rate3.append(-1)\n",
    "    else:\n",
    "        pred_rate3.append(1)\n",
    "\n",
    "FILTERED_SG_DF['sentitment'] = pred_rate3\n",
    "\n",
    "neg = len([i for i in pred_rate3 if i == -1])\n",
    "pos = len([i for i in pred_rate3 if i == 1])\n",
    "\n",
    "print(\"Total neg: {}\".format(neg))\n",
    "print(\"Total pos: {}\".format(pos))\n",
    "\n",
    "# Balancing positive and negative comments\n",
    "pos_row = []\n",
    "neg_row = []\n",
    "for idx, row in FILTERED_SG_DF.iterrows():\n",
    "    if row['sentitment'] == 1 and len(pos_row) < neg:\n",
    "        pos_row.append(row)\n",
    "    if row['sentitment'] == -1:\n",
    "        neg_row.append(row)\n",
    "\n",
    "FILTERED_SG_DF = pd.DataFrame(pos_row + neg_row)\n",
    "\n",
    "# Transform all documents into vectors\n",
    "X = vectorizer.transform(FILTERED_SG_DF['user_review'])\n",
    "Y = FILTERED_SG_DF['sentitment']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Train size: {}\".format(X_train.shape))\n",
    "print(\"Test size: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are 8x more positive comment than negative ones. We are going to rebalance them and remove 3 stars rating comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>-1</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred     -1   1  All\n",
       "actuals             \n",
       "-1       29   7   36\n",
       "1         2  30   32\n",
       "All      31  37   68"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model = MultinomialNB(alpha=1)\n",
    "nb_model.fit(X_train, Y_train)\n",
    "\n",
    "nb_train_preds = nb_model.predict(X_train)\n",
    "preds = nb_model.predict(X_test)\n",
    "\n",
    "confm = pd.crosstab(Y_test, preds, rownames= ['actuals'], colnames=['pred'], margins=True)\n",
    "confm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 30\n",
      "FP: 7\n",
      "FN: 2\n",
      "TN: 29\n",
      "\n",
      "Precision: 0.811\n",
      "Recall: 0.938\n",
      "Specificity: 0.806\n",
      "F1: 0.87\n"
     ]
    }
   ],
   "source": [
    "print(\"TP: {}\".format(confm[1][1]))\n",
    "print(\"FP: {}\".format(confm[1][-1]))\n",
    "print(\"FN: {}\".format(confm[-1][1]))\n",
    "print(\"TN: {}\".format(confm[-1][-1]))\n",
    "print()\n",
    "\n",
    "precision = (confm[1][1]) / (confm[1][1] + (confm[1][-1]))\n",
    "print (\"Precision: {}\".format(str(round(precision, 3))))\n",
    "\n",
    "recall = (confm[1][1]) / (confm[1][1] + (confm[-1][1]))\n",
    "print (\"Recall: {}\".format(str(round(recall, 3))))\n",
    "\n",
    "specificity = (confm[-1][-1] / (confm[-1][-1] + confm[1][-1]))\n",
    "print (\"Specificity: {}\".format(str(round(specificity, 3))))\n",
    "\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "print(\"F1: {}\".format(str(round(f1, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After balancing the training and dataset, specificity improve significately. As for F1 score it only has a minor drop of 0.01. This shows that skewed dataset can greatly affect our model and ambiguity labels can affect our model too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
