{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['content', 'date', 'tags', 'title', 'url'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "\n",
    "with open('train_new.json') as f:\n",
    "    data1=json.load(f)\n",
    "\n",
    "data1.keys()\n",
    "#pprint(data)\n",
    "\n",
    "with open('test_new.json') as f:\n",
    "    data2=json.load(f)\n",
    "\n",
    "data1.keys()\n",
    "#pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  Approximately 1.6 million model-year 2015-18 F...\n",
      "1  Approximately 192,000 model-year 2016-18 Toyot...\n",
      "2  Nobody likes when things don't work the way th...\n",
      "3  Approximately 19,400 model-year 2012 Toyota Av...\n",
      "4  Approximately 8,400 model-year 2017-18 Volkswa...\n",
      "(2815, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2815, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prepare text data\n",
    "\n",
    "import pandas as pd\n",
    "set=[]\n",
    "for k,v in data1['content'].items():\n",
    "    #print(k)\n",
    "    #print(v)\n",
    "    set.append(v)\n",
    "\n",
    "for k,v in data2['content'].items():\n",
    "    #print(k)\n",
    "    #print(v)\n",
    "    set.append(v)\n",
    "    \n",
    "df1=pd.DataFrame(set)\n",
    "print(df1.head())\n",
    "print(df1.shape)    \n",
    "    \n",
    "df1=pd.DataFrame(set)\n",
    "df1.head()\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.columns=['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Approximately 1.6 million model-year 2015-18 F...\n",
       "1       Approximately 192,000 model-year 2016-18 Toyot...\n",
       "2       Nobody likes when things don't work the way th...\n",
       "3       Approximately 19,400 model-year 2012 Toyota Av...\n",
       "4       Approximately 8,400 model-year 2017-18 Volkswa...\n",
       "5       Approximately 9,000 model-year 2018 Mercedes-B...\n",
       "6       Approximately 12,800 model-year 2018 Jeep Rene...\n",
       "7       Approximately 154,000 model-year 2018-19 Dodge...\n",
       "8       Approximately 90 model-year 2018 Ford Edge and...\n",
       "9       Approximately 49,000 model-year 2012-15 Ford F...\n",
       "10      Approximately 470 model-year 2018 Mercedes-Ben...\n",
       "11      Approximately 3,300 model-year 2018 Genesis G8...\n",
       "12      Approximately 300 model-year 2019 Subaru Ascen...\n",
       "13      Approximately 65 model-year 2018 Chrysler Paci...\n",
       "14      12 model-year 2018 BMW 540d xDrive sedans\\n Th...\n",
       "15      's 2018 Volkswagen Atlas, a long-term test veh...\n",
       "16      Nobody likes when things don't work the way th...\n",
       "17      As the owner of a Mercedes-AMG G65, you make y...\n",
       "18      Given the sheer thrill of driving an 840-horse...\n",
       "19      Approximately 10,800 model-year 2017-18 Merced...\n",
       "20      Approximately 5,600 model-year 2018-19 BMW X3 ...\n",
       "21      Approximately 33,100 model-year 2015-18 Volksw...\n",
       "22      If you're driving a model-year 2015 or 2016 La...\n",
       "23      Approximately 6,600 model-year 2013-15 Audi S8...\n",
       "24      Approximately 680 model-year 2018 Mazda CX-5 S...\n",
       "25      Getting a recall notice for your vehicle isn't...\n",
       "26      In the time it will take me to write this aler...\n",
       "27      Approximately 504,000 model-year 2013-14 Ford ...\n",
       "28      Approximately 700 model-year 2017-18 Porsche P...\n",
       "29      Approximately 1,800 model-year 2018 Dodge Jour...\n",
       "                              ...                        \n",
       "2785    Nissan and Infiniti are the latest automakers ...\n",
       "2786    In advertisements that help sell thousands of ...\n",
       "2787    Faulty Takata airbag inflators keep taking the...\n",
       "2788    General Motors has announced yet another spraw...\n",
       "2789    General Motors issued six more recalls on Wedn...\n",
       "2790    The ignition switch defects that engulfed Gene...\n",
       "2791    The public might associated ignition switch re...\n",
       "2792    At this point, there's little question that Ge...\n",
       "2793    General Motors has issued a stop-sale order on...\n",
       "2794    Kia is recalling certain 2014 models of its ad...\n",
       "2795    You may remember that Jeep's unusual fix for t...\n",
       "2796    Investigations into the General Motors ignitio...\n",
       "2797    We just can't seem to get away from recalls in...\n",
       "2798    DETROIT (AP) - BMW is expanding a recall of it...\n",
       "2799    It's barely arrived in dealerships, and alread...\n",
       "2800    Mercedes-Benz is the latest automaker to be af...\n",
       "2801    Investigators at the National Highway Traffic ...\n",
       "2802    Dodge and Jeep are announcing recalls of a tot...\n",
       "2803    The Acura ILX just can't seem to catch a break...\n",
       "2804    Most of the recalls we cover involve four-whee...\n",
       "2805    \"It will be done through cash on hand, no insu...\n",
       "2806    Volkswagen is issuing a stop-sale and recall f...\n",
       "2807    Ford is announcing six separate recalls for a ...\n",
       "2808    Through the first six months of 2014, General ...\n",
       "2809    Hyundai is recalling 58,000 Elantra Touring wa...\n",
       "2810    The scope of the problem with the faulty airba...\n",
       "2811    Lotus is recalling 860 vehicles after discover...\n",
       "2812    Jeep's saga with the National Traffic Safety A...\n",
       "2813    Subaru is recalling 660,238 vehicles located i...\n",
       "2814    No, that headline isn't a typo â€“ General Motor...\n",
       "Name: a, Length: 2815, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df1['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "mystopwords=stopwords.words(\"English\") + ['one', 'become', 'get', 'make', 'take', 'recall','said', 'say', 'could', 'nhtsa', 'n\\'t', 'may', 'vehicle', 'car']\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "def pre_process(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens=[ WNlemma.lemmatize(t.lower()) for t in tokens]\n",
    "    tokens=[ t for t in tokens if t not in mystopwords]\n",
    "    tokens = [ t for t in tokens if len(t) >= 3 ]\n",
    "    text_after_process=\" \".join(tokens)\n",
    "    return(text_after_process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process data\n",
    "\n",
    "toks_train = df1['a'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2815, 2500)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tfidf matrix\n",
    "vectorizer = TfidfVectorizer(max_features=2500,\n",
    "                              stop_words=mystopwords,\n",
    "                             use_idf=True)\n",
    "X = vectorizer.fit_transform(toks_train)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVD to reduce dimensions\n",
    "svd = TruncatedSVD(500)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "X_lsa = lsa.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step: 75%\n"
     ]
    }
   ],
   "source": [
    "# Check how much variance is explained\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 212 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=1000,\n",
       "    n_clusters=5, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cluster analysis\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "#random_state=4321\n",
    "km3 = KMeans(n_clusters=5, init='k-means++', max_iter=1000, n_init=1)\n",
    "%time km3.fit(X_lsa)\n",
    "\n",
    "#â€˜k-means++â€™ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.\n",
    "#n_init : int, default: 10. Number of time the k-means algorithm will be run with different centroid seeds.\n",
    "#Maximum number of iterations of the k-means algorithm for a single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient for 3 clusters: 0.024\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Silhouette score range (-1 to 1)\n",
    "\n",
    "print(\"Silhouette Coefficient for 3 clusters: %0.3f\"\n",
    "      % metrics.silhouette_score(X_lsa, km3.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: brake hyundai safety sonata owner model pedal parking kia fluid\n",
      "Cluster 1: ford escape lincoln plant fuel fusion door 2013 assembly 2014\n",
      "Cluster 2: takata inflator honda airbag airbags million side passenger mazda automaker\n",
      "Cluster 3: model safety fuel owner year chrysler problem seat dealer engine\n",
      "Cluster 4: toyota lexus pedal acceleration prius million 2010 automaker unintended issue\n"
     ]
    }
   ],
   "source": [
    "#Generate terms for each cluster\n",
    "\n",
    "def print_terms(cm, num):\n",
    "    original_space_centroids = svd.inverse_transform(cm.cluster_centers_)\n",
    "    order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(num):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()\n",
    "\n",
    "print_terms(km3, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
